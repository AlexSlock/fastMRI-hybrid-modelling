{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b8270fc",
   "metadata": {},
   "source": [
    "# VGG Loss Comparison and Efficiency Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "feb505ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/DATASERVER/MIC/GENERAL/STUDENTS/aslock2/conda/envs/DL_MRI_reconstruction_baselines/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import time\n",
    "from torchvision.models import vgg19\n",
    "from torchvision.transforms import Compose, ToTensor, CenterCrop, Normalize, Lambda\n",
    "from pathlib import Path\n",
    "import pathlib\n",
    "import h5py\n",
    "from fastmri.data import transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a842e31e",
   "metadata": {},
   "source": [
    "## Original VGG Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41f6d497",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "preprocess_orig = Compose([\n",
    "    ToTensor(),\n",
    "    CenterCrop((224, 224)),\n",
    "    Lambda(lambda x: x.repeat(3, 1, 1) if x.size(0)==1 else x),\n",
    "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "def vgg_loss_orig(gt: np.ndarray, pred: np.ndarray) -> np.ndarray:\n",
    "    # Load the pre-trained VGG19 model\n",
    "    vgg = vgg19(pretrained=True).features\n",
    "\n",
    "    # Remove the last max pooling layer to get the feature maps\n",
    "    vgg = torch.nn.Sequential(*list(vgg.children())[:-1])\n",
    "    \n",
    "    # mathijs did not use eval() here, but it is generally a good practice\n",
    "    # vgg.eval()  # Set the model to evaluation mode\n",
    "\n",
    "    gt = gt * 255\n",
    "    pred = pred * 255\n",
    "\n",
    "    losses = []\n",
    "\n",
    "    for gt_img, pred_img in zip(gt, pred):\n",
    "        gt_tensor = preprocess_orig(gt_img).unsqueeze(0)\n",
    "        pred_tensor = preprocess_orig(pred_img).unsqueeze(0)\n",
    "\n",
    "        gt_feat = vgg(gt_tensor)\n",
    "        pred_feat = vgg(pred_tensor)\n",
    "\n",
    "        loss = torch.nn.functional.mse_loss(gt_feat, pred_feat)\n",
    "        losses.append(loss)\n",
    "\n",
    "    # Average the losses across all images in the batch\n",
    "    avg_loss = torch.mean(torch.stack(losses))\n",
    "\n",
    "    return avg_loss.detach().cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e174b760",
   "metadata": {},
   "source": [
    "## Optimized VGG Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17f20a3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: 0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# Set the default GPU to GPU # \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"  # Force GPU number\n",
    "print(f\"Using GPU: {os.environ['CUDA_VISIBLE_DEVICES']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9186b1f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "preprocess_opt = Compose([\n",
    "    ToTensor(),\n",
    "    CenterCrop((224, 224)),\n",
    "    Lambda(lambda x: x.repeat(3, 1, 1) if x.size(0) == 1 else x),\n",
    "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "vgg_model = vgg19(pretrained=True).features[:36].to(device).eval()\n",
    "\n",
    "@torch.no_grad()\n",
    "def vgg_loss_opt(gt: np.ndarray, pred: np.ndarray) -> np.ndarray:\n",
    "    gt = gt * 255\n",
    "    pred = pred * 255\n",
    "\n",
    "    losses = []\n",
    "\n",
    "    for gt_img, pred_img in zip(gt, pred):\n",
    "        gt_tensor = preprocess_opt(gt_img).unsqueeze(0).to(device)\n",
    "        pred_tensor = preprocess_opt(pred_img).unsqueeze(0).to(device)\n",
    "\n",
    "        gt_feat = vgg_model(gt_tensor)\n",
    "        pred_feat = vgg_model(pred_tensor)\n",
    "\n",
    "        loss = torch.nn.functional.mse_loss(gt_feat, pred_feat)\n",
    "        losses.append(loss)\n",
    "\n",
    "    return torch.stack(losses).mean().item()\n",
    "    #return torch.stack(losses).mean().cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913d5946",
   "metadata": {},
   "source": [
    "## Generate Dummy Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ec538cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def determine_and_apply_mask(target, recons, tgt_file):\n",
    "    \"\"\"\n",
    "    processes two reconstruction files and applies a mask to \n",
    "    the target and reconstructed images based on the intersection of \n",
    "    non-zero values of 2 != reconstructions (sense and CS).\n",
    "    => goal: only evaluate where they have meaningful values \n",
    "    Args:\n",
    "        target (np.ndarray): ground truth image\n",
    "        recons (np.ndarray): reconstructed image\n",
    "        tgt_file (pathlib.Path): path to the target file\n",
    "    \"\"\"\n",
    "    # define the base paths for sense + CS reconstructions\n",
    "    reconstruction_sense_path_string = '/DATASERVER/MIC/GENERAL/STUDENTS/aslock2/Results/Sense/'\n",
    "    reconstruction_CS_path_string = '/DATASERVER/MIC/GENERAL/STUDENTS/aslock2/Results/CS/'\n",
    "    # Construct full pahts by appending target file name\n",
    "    reconstruction_sense_path = pathlib.Path(reconstruction_sense_path_string) / tgt_file.name\n",
    "    reconstruction_CS_path = pathlib.Path(reconstruction_CS_path_string) / tgt_file.name\n",
    "    # Read reconstruction files\n",
    "    reconstruction_sense = h5py.File(reconstruction_sense_path, 'r')\n",
    "    reconstruction_CS = h5py.File(reconstruction_CS_path, 'r')\n",
    "    reconstruction_sense = reconstruction_sense['reconstruction']\n",
    "    reconstruction_CS = reconstruction_CS['reconstruction']\n",
    "    # Convert to numpy arrays\n",
    "    reconstruction_sense = np.array(reconstruction_sense)\n",
    "    reconstruction_CS = np.array(reconstruction_CS)\n",
    "    # Crop the reconstructions to the same size as the target\n",
    "    reconstruction_sense = transforms.center_crop(reconstruction_sense, (target.shape[-1], target.shape[-1]))\n",
    "    reconstruction_CS = transforms.center_crop(reconstruction_CS, (target.shape[-1], target.shape[-1]))\n",
    "    # Create bitmasks where non-zero values in the reconstructions are marked as 1, and zero values are marked as 0.\n",
    "    sense_bitmask = np.ones_like(reconstruction_sense)\n",
    "    sense_bitmask = np.where(reconstruction_sense != 0, sense_bitmask, 0).astype(int)\n",
    "    CS_bitmask = np.ones_like(reconstruction_CS)\n",
    "    CS_bitmask = np.where(reconstruction_CS != 0, CS_bitmask, 0).astype(int)\n",
    "    # Create an intersection mask where the non-zero values in the sense and CS reconstructions overlap\n",
    "    intersection_mask = CS_bitmask & sense_bitmask\n",
    "    # Apply the intersection mask to the target and reconstructed images\n",
    "    gt = np.where(intersection_mask == 1, target, 0)\n",
    "    pred = np.where(intersection_mask == 1, recons, 0)\n",
    "        # If the value in intersection_mask is 1, the corresponding value from target is retained.\n",
    "        # If the value in intersection_mask is 0, the corresponding value in gt is set to 0.\n",
    "    return gt, pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8779a898",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using target file: /DATASERVER/MIC/SHARED/NYU_FastMRI/Preprocessed/multicoil_test_full/file_brain_AXT2_202_2020356.h5\n",
      "Using prediction file: /DATASERVER/MIC/GENERAL/STUDENTS/aslock2/Results/CSUNet/reconstructions/file_brain_AXT2_202_2020356.h5\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# # Dummy grayscale test data (batch of 3 images)\n",
    "# gt = np.random.rand(3, 256, 256).astype(np.float32)\n",
    "# pred = gt + np.random.normal(0, 0.01, size=gt.shape).astype(np.float32)\n",
    "\n",
    "# Generate real data:\n",
    "target_paths = [Path('/DATASERVER/MIC/SHARED/NYU_FastMRI/Preprocessed/multicoil_test_full/'), \n",
    "Path('/DATASERVER/MIC/SHARED/NYU_FastMRI/Knee/multicoil_val/')]\n",
    "predictions_path = Path('/DATASERVER/MIC/GENERAL/STUDENTS/aslock2/Results/CSUNet/reconstructions/')\n",
    "\n",
    "for pred_file in predictions_path.iterdir():\n",
    "  ###  find matching target file (knee or brain)\n",
    "  tgt_file = None\n",
    "  for target_dir in target_paths:\n",
    "      candidate = target_dir / pred_file.name\n",
    "      if candidate.exists():\n",
    "          tgt_file = candidate\n",
    "          break\n",
    "  assert tgt_file is not None, f\"Target file not found for {pred_file.name}\"\n",
    "  break\n",
    "print(f\"Using target file: {tgt_file}\")\n",
    "print(f\"Using prediction file: {pred_file}\")\n",
    "\n",
    "with h5py.File(tgt_file, \"r\") as target, h5py.File(pred_file, \"r\") as recons:\n",
    "   \n",
    "  # select target and reconstruction\n",
    "  target = target[\"reconstruction_rss\"][()] # \"reconstruction_rss\" of target files exists in multicoil_test_full set!\n",
    "  recons = recons[\"reconstruction\"][()]\n",
    "\n",
    "  # center crop the images to the size of the target\n",
    "  target = transforms.center_crop(\n",
    "      target, (target.shape[-1], target.shape[-1])\n",
    "  )\n",
    "  recons = transforms.center_crop(\n",
    "      recons, (target.shape[-1], target.shape[-1])\n",
    "  )\n",
    "  # apply non-zero mask to target and reconstruction\n",
    "  gt, pred = determine_and_apply_mask(target, recons, tgt_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a8ed04",
   "metadata": {},
   "source": [
    "## Compare Results and Timing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "98d95a8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original VGG Loss:   0.048915944993\n",
      "Optimized VGG Loss: 0.048915915191\n",
      "Difference:          0.000000029802\n",
      "Original Time:       18.4493 seconds\n",
      "Optimized Time:      0.1753 seconds\n",
      "Speedup:             105.24x\n"
     ]
    }
   ],
   "source": [
    "\n",
    "start_orig = time.time()\n",
    "loss_orig = vgg_loss_orig(gt, pred)\n",
    "time_orig = time.time() - start_orig\n",
    "\n",
    "start_opt = time.time()\n",
    "loss_opt = vgg_loss_opt(gt, pred)\n",
    "time_opt = time.time() - start_opt\n",
    "\n",
    "print(f\"Original VGG Loss:   {loss_orig:.12f}\")\n",
    "print(f\"Optimized VGG Loss: {loss_opt:.12f}\")\n",
    "print(f\"Difference:          {abs(loss_orig - loss_opt):.12f}\")\n",
    "print(f\"Original Time:       {time_orig:.4f} seconds\")\n",
    "print(f\"Optimized Time:      {time_opt:.4f} seconds\")\n",
    "print(f\"Speedup:             {time_orig / time_opt:.2f}x\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL_MRI_reconstruction_baselines",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
